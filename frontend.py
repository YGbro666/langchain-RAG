import streamlit as st
import requests
import os

# streamlit run frontend.py --server.port=8082
# è®¾ç½®é¡µé¢æ ‡é¢˜
st.set_page_config(page_title="LLM Chat Interface", layout="wide")

# åˆå§‹åŒ–session stateç”¨äºå­˜å‚¨å¯¹è¯å†å²
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

BACKEND_URL = "http://localhost:8084"
LANGCHAIN_URL = "http://localhost:8083"
Ollama_URL = "http://localhost:11434"

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown(
    """
<style>
.user-message {
    background-color: #e6f3ff;
    padding: 15px;
    border-radius: 10px;
    margin: 5px 0;
}
.ai-message {
    background-color: #f0f0f0;
    padding: 15px;
    border-radius: 10px;
    margin: 5px 0;
}
.chat-container {
    margin-bottom: 20px;
}
</style>
""",
    unsafe_allow_html=True,
)


def load_model(model_name):
    """åŠ è½½æ¨¡å‹"""
    try:
        # Ollama API è¦æ±‚çš„è´Ÿè½½æ ¼å¼
        payload = {"name": model_name, "stream": False}

        # ä½¿ç”¨ Ollama çš„æ¨¡å‹åŠ è½½ API
        response = requests.post(
            f"{Ollama_URL}/api/pull",  # Ollama çš„æ‹‰å–æ¨¡å‹ API ç«¯ç‚¹
            json=payload,
            timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
        )

        if response.status_code == 200:
            return {"status": "success", "message": f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ"}
        else:
            return {"status": "error", "message": f"æ¨¡å‹åŠ è½½å¤±è´¥: {response.text}"}

    except requests.exceptions.ConnectionError:
        return {
            "status": "error",
            "message": "æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ",
        }
    except requests.exceptions.Timeout:
        return {
            "status": "error",
            "message": "åŠ è½½æ¨¡å‹è¶…æ—¶ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹è¾ƒå¤§æˆ–ç½‘ç»œé—®é¢˜ï¼Œè¯·ç¨åé‡è¯•",
        }
    except Exception as e:
        return {"status": "error", "message": f"å‘ç”Ÿé”™è¯¯: {str(e)}"}


def chat_with_model(prompt, max_tokens=512, temperature=1.0):
    """ä¸æ¨¡å‹å¯¹è¯ï¼ˆä½¿ç”¨ agent æ¨¡å¼ï¼‰"""
    try:
        response = requests.post(
            f"{LANGCHAIN_URL}/agent",  # æ”¹ä¸ºä½¿ç”¨ agent ç«¯ç‚¹
            json={
                "prompt": prompt,
                "max_tokens": max_tokens,
                "temperature": temperature,
            },
            timeout=300,  # æ·»åŠ  5 åˆ†é’Ÿè¶…æ—¶
        )
        return response.json()
    except requests.exceptions.Timeout:
        return {
            "status": "error",
            "message": "è¯·æ±‚è¶…æ—¶ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºé—®é¢˜è¾ƒå¤æ‚æˆ–éœ€è¦å¤šä¸ªå·¥å…·å¤„ç†ï¼Œè¯·ç¨åé‡è¯•",
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}


def check_model_status():
    """æ£€æŸ¥æ¨¡å‹çŠ¶æ€"""
    try:
        response = requests.get(f"{BACKEND_URL}/model_status")
        return response.json()
    except Exception as e:
        return {"status": "error", "message": str(e)}


# é¡µé¢æ ‡é¢˜
st.title("ğŸ¤– LLM Chat Interface")

# ä¾§è¾¹æ  - æ¨¡å‹é€‰æ‹©å’ŒåŠ è½½
with st.sidebar:
    st.header("æ¨¡å‹è®¾ç½®")

    # ä¿®æ”¹ä¸ºOllamaæ”¯æŒçš„æ¨¡å‹é€‰é¡¹
    model_options = {
        "Qwen-14B": "qwen2.5:14b",
        "Qwen-7B": "qwen2.5:7b",
        "Llama2": "llama2",
        "Mistral": "mistral",
        "Neural-Chat": "neural-chat",
        "CodeLlama": "codellama",
    }

    selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", options=list(model_options.keys()))

    # åŠ è½½æ¨¡å‹æŒ‰é’®
    if st.button("åŠ è½½æ¨¡å‹"):
        with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…..."):
            result = load_model(model_options[selected_model])
            if isinstance(result, dict):
                if result["status"] == "success":
                    st.success(result["message"])
                else:
                    st.error(result.get("message", "æœªçŸ¥é”™è¯¯"))
            else:
                st.error("æ— æ•ˆçš„å“åº”æ ¼å¼")

    # æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€
    st.subheader("æ¨¡å‹çŠ¶æ€")
    status = check_model_status()
    st.write(f"æ¨¡å‹å·²åŠ è½½: {'âœ…' if status.get('model_loaded') else 'âŒ'}")

# ä¸»ç•Œé¢ - å¯¹è¯åŒºåŸŸ
st.header("å¯¹è¯åŒºåŸŸ")

# æ˜¾ç¤ºå¯¹è¯å†å²
st.markdown("### å¯¹è¯å†å²")
for message in st.session_state.chat_history:
    if message["role"] == "user":
        st.markdown(
            f'<div class="user-message">ğŸ‘¤ ç”¨æˆ·: {message["content"]}</div>',
            unsafe_allow_html=True,
        )
    else:
        st.markdown(
            f'<div class="ai-message">ğŸ¤– AI: {message["content"]}</div>',
            unsafe_allow_html=True,
        )

# æ¸…é™¤å†å²è®°å½•æŒ‰é’®
if st.button("æ¸…é™¤å†å²è®°å½•"):
    st.session_state.chat_history = []
    st.rerun()

# å‚æ•°è®¾ç½®
col1, col2 = st.columns(2)
with col1:
    max_tokens = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 64, 2048, 512)
with col2:
    temperature = st.slider("æ¸©åº¦ (éšæœºæ€§)", 0.1, 2.0, 1.0)

# ç”¨æˆ·è¾“å…¥
user_input = st.text_area("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:", height=100)

# Agentå¯¹è¯æŒ‰é’®
if st.button("Agentå¯¹è¯"):
    if not user_input.strip():
        st.warning("è¯·è¾“å…¥é—®é¢˜ï¼")
    else:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
        st.session_state.chat_history.append({"role": "user", "content": user_input})

        with st.spinner("æ­£åœ¨æ€è€ƒå¹¶å¤„ç†æ‚¨çš„é—®é¢˜..."):
            response = chat_with_model(
                user_input, max_tokens=max_tokens, temperature=temperature
            )
            if response.get("status") == "success":
                # æ·»åŠ AIå›å¤åˆ°å†å²è®°å½•
                st.session_state.chat_history.append(
                    {"role": "assistant", "content": response["response"]}
                )
                # é‡æ–°åŠ è½½é¡µé¢ä»¥æ˜¾ç¤ºæ–°æ¶ˆæ¯
                st.rerun()
            else:
                st.error(f"é”™è¯¯: {response.get('message', 'æœªçŸ¥é”™è¯¯')}")

# æ›´æ–°é¡µé¢åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.markdown("""
ğŸ’¡ æç¤ºï¼š
- å…ˆåœ¨å·¦ä¾§é€‰æ‹©å¹¶åŠ è½½æ¨¡å‹
- Agentå¯¹è¯ï¼šæ™ºèƒ½ä»£ç†ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§å·¥å…·
- å¯ä»¥è°ƒæ•´ç”Ÿæˆé•¿åº¦å’Œéšæœºæ€§å‚æ•°
""")
